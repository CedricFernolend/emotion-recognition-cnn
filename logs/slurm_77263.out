==========================================
Job started at: Fri Jan 30 19:58:35 CET 2026
Running on node: aplit.cip.ifi.lmu.de
Job ID: 77263
GPU allocated: 
Model versions to train: v1 v2 v4
==========================================
Working directory: /home/f/fernolend/emotion-recognition-cnn
Creating virtual environment...
Virtual environment activated: /home/f/fernolend/emotion-recognition-cnn/venv_cluster/bin/python
Installing requirements...
Requirement already satisfied: pip in ./venv_cluster/lib/python3.12/site-packages (24.0)
Collecting pip
  Using cached pip-25.3-py3-none-any.whl.metadata (4.7 kB)
Using cached pip-25.3-py3-none-any.whl (1.8 MB)
Installing collected packages: pip
  Attempting uninstall: pip
    Found existing installation: pip 24.0
    Uninstalling pip-24.0:
      Successfully uninstalled pip-24.0
Successfully installed pip-25.3
Collecting torch>=2.0.0 (from -r requirements.txt (line 1))
  Using cached torch-2.10.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (31 kB)
Collecting torchvision>=0.15.0 (from -r requirements.txt (line 2))
  Using cached torchvision-0.25.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (5.4 kB)
Collecting numpy>=1.24.0 (from -r requirements.txt (line 3))
  Using cached numpy-2.4.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (6.6 kB)
Collecting Pillow>=10.0.0 (from -r requirements.txt (line 4))
  Using cached pillow-12.1.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (8.8 kB)
Collecting scikit-learn>=1.3.0 (from -r requirements.txt (line 5))
  Using cached scikit_learn-1.8.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (11 kB)
Collecting matplotlib>=3.7.0 (from -r requirements.txt (line 6))
  Using cached matplotlib-3.10.8-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (52 kB)
Collecting seaborn>=0.12.0 (from -r requirements.txt (line 7))
  Using cached seaborn-0.13.2-py3-none-any.whl.metadata (5.4 kB)
Collecting tqdm>=4.65.0 (from -r requirements.txt (line 8))
  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)
Collecting opencv-python>=4.8.0 (from -r requirements.txt (line 9))
  Using cached opencv_python-4.13.0.90-cp37-abi3-manylinux_2_28_x86_64.whl.metadata (19 kB)
Collecting pytest>=7.0.0 (from -r requirements.txt (line 10))
  Using cached pytest-9.0.2-py3-none-any.whl.metadata (7.6 kB)
Collecting pytest-cov>=4.0.0 (from -r requirements.txt (line 11))
  Using cached pytest_cov-7.0.0-py3-none-any.whl.metadata (31 kB)
Collecting filelock (from torch>=2.0.0->-r requirements.txt (line 1))
  Using cached filelock-3.20.3-py3-none-any.whl.metadata (2.1 kB)
Collecting typing-extensions>=4.10.0 (from torch>=2.0.0->-r requirements.txt (line 1))
  Using cached typing_extensions-4.15.0-py3-none-any.whl.metadata (3.3 kB)
Collecting setuptools (from torch>=2.0.0->-r requirements.txt (line 1))
  Downloading setuptools-80.10.2-py3-none-any.whl.metadata (6.6 kB)
Collecting sympy>=1.13.3 (from torch>=2.0.0->-r requirements.txt (line 1))
  Using cached sympy-1.14.0-py3-none-any.whl.metadata (12 kB)
Collecting networkx>=2.5.1 (from torch>=2.0.0->-r requirements.txt (line 1))
  Using cached networkx-3.6.1-py3-none-any.whl.metadata (6.8 kB)
Collecting jinja2 (from torch>=2.0.0->-r requirements.txt (line 1))
  Using cached jinja2-3.1.6-py3-none-any.whl.metadata (2.9 kB)
Collecting fsspec>=0.8.5 (from torch>=2.0.0->-r requirements.txt (line 1))
  Using cached fsspec-2026.1.0-py3-none-any.whl.metadata (10 kB)
Collecting cuda-bindings==12.9.4 (from torch>=2.0.0->-r requirements.txt (line 1))
  Using cached cuda_bindings-12.9.4-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (2.6 kB)
Collecting nvidia-cuda-nvrtc-cu12==12.8.93 (from torch>=2.0.0->-r requirements.txt (line 1))
  Using cached nvidia_cuda_nvrtc_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)
Collecting nvidia-cuda-runtime-cu12==12.8.90 (from torch>=2.0.0->-r requirements.txt (line 1))
  Using cached nvidia_cuda_runtime_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)
Collecting nvidia-cuda-cupti-cu12==12.8.90 (from torch>=2.0.0->-r requirements.txt (line 1))
  Using cached nvidia_cuda_cupti_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)
Collecting nvidia-cudnn-cu12==9.10.2.21 (from torch>=2.0.0->-r requirements.txt (line 1))
  Using cached nvidia_cudnn_cu12-9.10.2.21-py3-none-manylinux_2_27_x86_64.whl.metadata (1.8 kB)
Collecting nvidia-cublas-cu12==12.8.4.1 (from torch>=2.0.0->-r requirements.txt (line 1))
  Using cached nvidia_cublas_cu12-12.8.4.1-py3-none-manylinux_2_27_x86_64.whl.metadata (1.7 kB)
Collecting nvidia-cufft-cu12==11.3.3.83 (from torch>=2.0.0->-r requirements.txt (line 1))
  Using cached nvidia_cufft_cu12-11.3.3.83-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)
Collecting nvidia-curand-cu12==10.3.9.90 (from torch>=2.0.0->-r requirements.txt (line 1))
  Using cached nvidia_curand_cu12-10.3.9.90-py3-none-manylinux_2_27_x86_64.whl.metadata (1.7 kB)
Collecting nvidia-cusolver-cu12==11.7.3.90 (from torch>=2.0.0->-r requirements.txt (line 1))
  Using cached nvidia_cusolver_cu12-11.7.3.90-py3-none-manylinux_2_27_x86_64.whl.metadata (1.8 kB)
Collecting nvidia-cusparse-cu12==12.5.8.93 (from torch>=2.0.0->-r requirements.txt (line 1))
  Using cached nvidia_cusparse_cu12-12.5.8.93-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.8 kB)
Collecting nvidia-cusparselt-cu12==0.7.1 (from torch>=2.0.0->-r requirements.txt (line 1))
  Using cached nvidia_cusparselt_cu12-0.7.1-py3-none-manylinux2014_x86_64.whl.metadata (7.0 kB)
Collecting nvidia-nccl-cu12==2.27.5 (from torch>=2.0.0->-r requirements.txt (line 1))
  Using cached nvidia_nccl_cu12-2.27.5-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.0 kB)
Collecting nvidia-nvshmem-cu12==3.4.5 (from torch>=2.0.0->-r requirements.txt (line 1))
  Using cached nvidia_nvshmem_cu12-3.4.5-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.1 kB)
Collecting nvidia-nvtx-cu12==12.8.90 (from torch>=2.0.0->-r requirements.txt (line 1))
  Using cached nvidia_nvtx_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.8 kB)
Collecting nvidia-nvjitlink-cu12==12.8.93 (from torch>=2.0.0->-r requirements.txt (line 1))
  Using cached nvidia_nvjitlink_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)
Collecting nvidia-cufile-cu12==1.13.1.3 (from torch>=2.0.0->-r requirements.txt (line 1))
  Using cached nvidia_cufile_cu12-1.13.1.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)
Collecting triton==3.6.0 (from torch>=2.0.0->-r requirements.txt (line 1))
  Using cached triton-3.6.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.7 kB)
Collecting cuda-pathfinder~=1.1 (from cuda-bindings==12.9.4->torch>=2.0.0->-r requirements.txt (line 1))
  Using cached cuda_pathfinder-1.3.3-py3-none-any.whl.metadata (1.9 kB)
Collecting scipy>=1.10.0 (from scikit-learn>=1.3.0->-r requirements.txt (line 5))
  Using cached scipy-1.17.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (62 kB)
Collecting joblib>=1.3.0 (from scikit-learn>=1.3.0->-r requirements.txt (line 5))
  Using cached joblib-1.5.3-py3-none-any.whl.metadata (5.5 kB)
Collecting threadpoolctl>=3.2.0 (from scikit-learn>=1.3.0->-r requirements.txt (line 5))
  Using cached threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)
Collecting contourpy>=1.0.1 (from matplotlib>=3.7.0->-r requirements.txt (line 6))
  Using cached contourpy-1.3.3-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (5.5 kB)
Collecting cycler>=0.10 (from matplotlib>=3.7.0->-r requirements.txt (line 6))
  Using cached cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)
Collecting fonttools>=4.22.0 (from matplotlib>=3.7.0->-r requirements.txt (line 6))
  Using cached fonttools-4.61.1-cp312-cp312-manylinux1_x86_64.manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_5_x86_64.whl.metadata (114 kB)
Collecting kiwisolver>=1.3.1 (from matplotlib>=3.7.0->-r requirements.txt (line 6))
  Using cached kiwisolver-1.4.9-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (6.3 kB)
Collecting packaging>=20.0 (from matplotlib>=3.7.0->-r requirements.txt (line 6))
  Using cached packaging-26.0-py3-none-any.whl.metadata (3.3 kB)
Collecting pyparsing>=3 (from matplotlib>=3.7.0->-r requirements.txt (line 6))
  Using cached pyparsing-3.3.2-py3-none-any.whl.metadata (5.8 kB)
Collecting python-dateutil>=2.7 (from matplotlib>=3.7.0->-r requirements.txt (line 6))
  Using cached python_dateutil-2.9.0.post0-py2.py3-none-any.whl.metadata (8.4 kB)
Collecting pandas>=1.2 (from seaborn>=0.12.0->-r requirements.txt (line 7))
  Using cached pandas-3.0.0-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (79 kB)
Collecting iniconfig>=1.0.1 (from pytest>=7.0.0->-r requirements.txt (line 10))
  Using cached iniconfig-2.3.0-py3-none-any.whl.metadata (2.5 kB)
Collecting pluggy<2,>=1.5 (from pytest>=7.0.0->-r requirements.txt (line 10))
  Using cached pluggy-1.6.0-py3-none-any.whl.metadata (4.8 kB)
Collecting pygments>=2.7.2 (from pytest>=7.0.0->-r requirements.txt (line 10))
  Using cached pygments-2.19.2-py3-none-any.whl.metadata (2.5 kB)
Collecting coverage>=7.10.6 (from coverage[toml]>=7.10.6->pytest-cov>=4.0.0->-r requirements.txt (line 11))
  Downloading coverage-7.13.2-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl.metadata (8.5 kB)
Collecting six>=1.5 (from python-dateutil>=2.7->matplotlib>=3.7.0->-r requirements.txt (line 6))
  Using cached six-1.17.0-py2.py3-none-any.whl.metadata (1.7 kB)
Collecting mpmath<1.4,>=1.1.0 (from sympy>=1.13.3->torch>=2.0.0->-r requirements.txt (line 1))
  Using cached mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)
Collecting MarkupSafe>=2.0 (from jinja2->torch>=2.0.0->-r requirements.txt (line 1))
  Using cached markupsafe-3.0.3-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (2.7 kB)
Using cached torch-2.10.0-cp312-cp312-manylinux_2_28_x86_64.whl (915.7 MB)
Using cached cuda_bindings-12.9.4-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (12.2 MB)
Using cached nvidia_cublas_cu12-12.8.4.1-py3-none-manylinux_2_27_x86_64.whl (594.3 MB)
Using cached nvidia_cuda_cupti_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (10.2 MB)
Using cached nvidia_cuda_nvrtc_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (88.0 MB)
Using cached nvidia_cuda_runtime_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (954 kB)
Using cached nvidia_cudnn_cu12-9.10.2.21-py3-none-manylinux_2_27_x86_64.whl (706.8 MB)
Using cached nvidia_cufft_cu12-11.3.3.83-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (193.1 MB)
Using cached nvidia_cufile_cu12-1.13.1.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (1.2 MB)
Using cached nvidia_curand_cu12-10.3.9.90-py3-none-manylinux_2_27_x86_64.whl (63.6 MB)
Using cached nvidia_cusolver_cu12-11.7.3.90-py3-none-manylinux_2_27_x86_64.whl (267.5 MB)
Using cached nvidia_cusparse_cu12-12.5.8.93-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (288.2 MB)
Using cached nvidia_cusparselt_cu12-0.7.1-py3-none-manylinux2014_x86_64.whl (287.2 MB)
Using cached nvidia_nccl_cu12-2.27.5-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (322.3 MB)
Using cached nvidia_nvjitlink_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (39.3 MB)
Using cached nvidia_nvshmem_cu12-3.4.5-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (139.1 MB)
Using cached nvidia_nvtx_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89 kB)
Using cached triton-3.6.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (188.3 MB)
Using cached cuda_pathfinder-1.3.3-py3-none-any.whl (27 kB)
Using cached torchvision-0.25.0-cp312-cp312-manylinux_2_28_x86_64.whl (8.1 MB)
Using cached numpy-2.4.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (16.4 MB)
Using cached pillow-12.1.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (7.0 MB)
Using cached scikit_learn-1.8.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (8.9 MB)
Using cached matplotlib-3.10.8-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (8.7 MB)
Using cached seaborn-0.13.2-py3-none-any.whl (294 kB)
Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)
Using cached opencv_python-4.13.0.90-cp37-abi3-manylinux_2_28_x86_64.whl (72.9 MB)
Using cached pytest-9.0.2-py3-none-any.whl (374 kB)
Using cached pluggy-1.6.0-py3-none-any.whl (20 kB)
Using cached pytest_cov-7.0.0-py3-none-any.whl (22 kB)
Using cached contourpy-1.3.3-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (362 kB)
Downloading coverage-7.13.2-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl (253 kB)
Using cached cycler-0.12.1-py3-none-any.whl (8.3 kB)
Using cached fonttools-4.61.1-cp312-cp312-manylinux1_x86_64.manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_5_x86_64.whl (5.0 MB)
Using cached fsspec-2026.1.0-py3-none-any.whl (201 kB)
Using cached iniconfig-2.3.0-py3-none-any.whl (7.5 kB)
Using cached joblib-1.5.3-py3-none-any.whl (309 kB)
Using cached kiwisolver-1.4.9-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (1.5 MB)
Using cached networkx-3.6.1-py3-none-any.whl (2.1 MB)
Using cached packaging-26.0-py3-none-any.whl (74 kB)
Using cached pandas-3.0.0-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (10.9 MB)
Using cached pygments-2.19.2-py3-none-any.whl (1.2 MB)
Using cached pyparsing-3.3.2-py3-none-any.whl (122 kB)
Using cached python_dateutil-2.9.0.post0-py2.py3-none-any.whl (229 kB)
Using cached scipy-1.17.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (35.0 MB)
Using cached six-1.17.0-py2.py3-none-any.whl (11 kB)
Using cached sympy-1.14.0-py3-none-any.whl (6.3 MB)
Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)
Using cached threadpoolctl-3.6.0-py3-none-any.whl (18 kB)
Using cached typing_extensions-4.15.0-py3-none-any.whl (44 kB)
Using cached filelock-3.20.3-py3-none-any.whl (16 kB)
Using cached jinja2-3.1.6-py3-none-any.whl (134 kB)
Using cached markupsafe-3.0.3-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (22 kB)
Downloading setuptools-80.10.2-py3-none-any.whl (1.1 MB)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.1/1.1 MB 5.3 MB/s  0:00:00
Installing collected packages: nvidia-cusparselt-cu12, mpmath, typing-extensions, triton, tqdm, threadpoolctl, sympy, six, setuptools, pyparsing, pygments, pluggy, Pillow, packaging, nvidia-nvtx-cu12, nvidia-nvshmem-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufile-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, numpy, networkx, MarkupSafe, kiwisolver, joblib, iniconfig, fsspec, fonttools, filelock, cycler, cuda-pathfinder, coverage, scipy, python-dateutil, pytest, opencv-python, nvidia-cusparse-cu12, nvidia-cufft-cu12, nvidia-cudnn-cu12, jinja2, cuda-bindings, contourpy, scikit-learn, pytest-cov, pandas, nvidia-cusolver-cu12, matplotlib, torch, seaborn, torchvision

Successfully installed MarkupSafe-3.0.3 Pillow-12.1.0 contourpy-1.3.3 coverage-7.13.2 cuda-bindings-12.9.4 cuda-pathfinder-1.3.3 cycler-0.12.1 filelock-3.20.3 fonttools-4.61.1 fsspec-2026.1.0 iniconfig-2.3.0 jinja2-3.1.6 joblib-1.5.3 kiwisolver-1.4.9 matplotlib-3.10.8 mpmath-1.3.0 networkx-3.6.1 numpy-2.4.1 nvidia-cublas-cu12-12.8.4.1 nvidia-cuda-cupti-cu12-12.8.90 nvidia-cuda-nvrtc-cu12-12.8.93 nvidia-cuda-runtime-cu12-12.8.90 nvidia-cudnn-cu12-9.10.2.21 nvidia-cufft-cu12-11.3.3.83 nvidia-cufile-cu12-1.13.1.3 nvidia-curand-cu12-10.3.9.90 nvidia-cusolver-cu12-11.7.3.90 nvidia-cusparse-cu12-12.5.8.93 nvidia-cusparselt-cu12-0.7.1 nvidia-nccl-cu12-2.27.5 nvidia-nvjitlink-cu12-12.8.93 nvidia-nvshmem-cu12-3.4.5 nvidia-nvtx-cu12-12.8.90 opencv-python-4.13.0.90 packaging-26.0 pandas-3.0.0 pluggy-1.6.0 pygments-2.19.2 pyparsing-3.3.2 pytest-9.0.2 pytest-cov-7.0.0 python-dateutil-2.9.0.post0 scikit-learn-1.8.0 scipy-1.17.0 seaborn-0.13.2 setuptools-80.10.2 six-1.17.0 sympy-1.14.0 threadpoolctl-3.6.0 torch-2.10.0 torchvision-0.25.0 tqdm-4.67.1 triton-3.6.0 typing-extensions-4.15.0

==========================================
GPU Check
==========================================
PyTorch version: 2.10.0+cu128
CUDA available: True
CUDA version: 12.8
GPU count: 1
GPU name: NVIDIA GeForce RTX 2060 SUPER

==========================================
Starting training for model version: v1
Time: Fri Jan 30 20:05:58 CET 2026
==========================================
Using device: cuda
GPU: NVIDIA GeForce RTX 2060 SUPER

Training model version: v1
Model name: Baseline 3-Block CNN

==================================================
Configuration Summary
==================================================
  Learning Rate:     0.0003
  Weight Decay:      0.0
  Dropout Rate:      0.5
  Epochs (max):      60
  Early Stop:        10 epochs
  Class Weights:     True
  Label Smoothing:   0.0
  LR Scheduler:      False
  Model Save Path:   /home/f/fernolend/emotion-recognition-cnn/results/v1/models/emotion_model.pth
==================================================

Loading data...
Dataset splits: train=18996, val=4748, test=5945
Train batches: 594, Val batches: 149, Test batches: 186

Initializing model...
Total parameters: 1,813,510
Trainable parameters: 1,813,510

Computing class weights...
Class weights (inverse frequency):
  happiness: 0.548 (7215 samples)
  surprise: 1.248 (3171 samples)
  sadness: 0.819 (4830 samples)
  anger: 0.991 (3995 samples)
  disgust: 9.076 (436 samples)
  fear: 0.966 (4097 samples)
Using weighted CrossEntropyLoss with label smoothing (0.0)
Optimizer: Adam (lr=0.0003, weight_decay=0.0)
LR Scheduler: None

Training for up to 60 epochs (early stopping patience: 10)

Epoch 1/60 (lr: 0.000300)
Train: 23.20% (loss: 1.7640) | Val: 32.62% (loss: 1.7389) | Gap: -9.42%
New best model saved! Val Acc: 32.62%

Epoch 2/60 (lr: 0.000300)
Train: 31.90% (loss: 1.6707) | Val: 38.10% (loss: 1.5915) | Gap: -6.20%
New best model saved! Val Acc: 38.10%

Epoch 3/60 (lr: 0.000300)
Train: 38.91% (loss: 1.5753) | Val: 29.04% (loss: 1.6532) | Gap: 9.86%
No improvement (1/10)

Epoch 4/60 (lr: 0.000300)
Train: 43.07% (loss: 1.4928) | Val: 45.26% (loss: 1.6632) | Gap: -2.19%
New best model saved! Val Acc: 45.26%

Epoch 5/60 (lr: 0.000300)
Train: 44.74% (loss: 1.4301) | Val: 44.25% (loss: 1.3601) | Gap: 0.49%
No improvement (1/10)

Epoch 6/60 (lr: 0.000300)
Train: 45.88% (loss: 1.3727) | Val: 45.62% (loss: 1.3705) | Gap: 0.26%
New best model saved! Val Acc: 45.62%

Epoch 7/60 (lr: 0.000300)
Train: 47.20% (loss: 1.3438) | Val: 43.30% (loss: 1.3643) | Gap: 3.90%
No improvement (1/10)

Epoch 8/60 (lr: 0.000300)
Train: 49.12% (loss: 1.3104) | Val: 38.86% (loss: 1.4203) | Gap: 10.26%
No improvement (2/10)

Epoch 9/60 (lr: 0.000300)
Train: 49.46% (loss: 1.2770) | Val: 53.48% (loss: 1.2275) | Gap: -4.01%
New best model saved! Val Acc: 53.48%

Epoch 10/60 (lr: 0.000300)
Train: 50.70% (loss: 1.2533) | Val: 54.72% (loss: 1.2828) | Gap: -4.02%
New best model saved! Val Acc: 54.72%

Epoch 11/60 (lr: 0.000300)
Train: 51.46% (loss: 1.2338) | Val: 50.11% (loss: 1.3442) | Gap: 1.36%
No improvement (1/10)

Epoch 12/60 (lr: 0.000300)
Train: 52.28% (loss: 1.2074) | Val: 57.08% (loss: 1.2150) | Gap: -4.80%
New best model saved! Val Acc: 57.08%

Epoch 13/60 (lr: 0.000300)
Train: 53.34% (loss: 1.1894) | Val: 54.68% (loss: 1.1623) | Gap: -1.33%
No improvement (1/10)

Epoch 14/60 (lr: 0.000300)
Train: 53.53% (loss: 1.1837) | Val: 56.76% (loss: 1.1262) | Gap: -3.23%
No improvement (2/10)

Epoch 15/60 (lr: 0.000300)
Train: 54.58% (loss: 1.1624) | Val: 56.47% (loss: 1.1817) | Gap: -1.89%
No improvement (3/10)

Epoch 16/60 (lr: 0.000300)
Train: 54.84% (loss: 1.1519) | Val: 61.06% (loss: 1.1931) | Gap: -6.21%
New best model saved! Val Acc: 61.06%

Epoch 17/60 (lr: 0.000300)
Train: 55.80% (loss: 1.1232) | Val: 58.36% (loss: 1.0791) | Gap: -2.57%
No improvement (1/10)

Epoch 18/60 (lr: 0.000300)
Train: 56.72% (loss: 1.1011) | Val: 57.03% (loss: 1.1584) | Gap: -0.31%
No improvement (2/10)

Epoch 19/60 (lr: 0.000300)
Train: 56.74% (loss: 1.1033) | Val: 60.76% (loss: 1.1539) | Gap: -4.02%
No improvement (3/10)

Epoch 20/60 (lr: 0.000300)
Train: 57.62% (loss: 1.0750) | Val: 58.99% (loss: 1.1612) | Gap: -1.37%
No improvement (4/10)

Epoch 21/60 (lr: 0.000300)
Train: 58.66% (loss: 1.0698) | Val: 59.22% (loss: 1.0530) | Gap: -0.56%
No improvement (5/10)

Epoch 22/60 (lr: 0.000300)
Train: 58.68% (loss: 1.0561) | Val: 60.34% (loss: 1.0794) | Gap: -1.67%
No improvement (6/10)

Epoch 23/60 (lr: 0.000300)
Train: 58.53% (loss: 1.0503) | Val: 60.40% (loss: 1.0505) | Gap: -1.88%
No improvement (7/10)

Epoch 24/60 (lr: 0.000300)
Train: 59.65% (loss: 1.0197) | Val: 60.59% (loss: 1.0433) | Gap: -0.94%
No improvement (8/10)

Epoch 25/60 (lr: 0.000300)
Train: 59.44% (loss: 1.0340) | Val: 59.86% (loss: 1.0641) | Gap: -0.42%
No improvement (9/10)

Epoch 26/60 (lr: 0.000300)
Train: 60.32% (loss: 1.0101) | Val: 64.22% (loss: 0.9948) | Gap: -3.89%
New best model saved! Val Acc: 64.22%

Epoch 27/60 (lr: 0.000300)
Train: 60.85% (loss: 1.0077) | Val: 60.87% (loss: 1.0190) | Gap: -0.02%
No improvement (1/10)

Epoch 28/60 (lr: 0.000300)
Train: 60.89% (loss: 0.9811) | Val: 62.34% (loss: 0.9973) | Gap: -1.45%
No improvement (2/10)

Epoch 29/60 (lr: 0.000300)
Train: 60.53% (loss: 0.9928) | Val: 61.82% (loss: 0.9998) | Gap: -1.28%
No improvement (3/10)

Epoch 30/60 (lr: 0.000300)
Train: 61.29% (loss: 0.9739) | Val: 62.43% (loss: 0.9872) | Gap: -1.13%
No improvement (4/10)

Epoch 31/60 (lr: 0.000300)
Train: 61.89% (loss: 0.9623) | Val: 64.34% (loss: 1.0426) | Gap: -2.45%
New best model saved! Val Acc: 64.34%

Epoch 32/60 (lr: 0.000300)
Train: 62.05% (loss: 0.9557) | Val: 64.28% (loss: 1.0040) | Gap: -2.23%
No improvement (1/10)

Epoch 33/60 (lr: 0.000300)
Train: 62.75% (loss: 0.9413) | Val: 63.86% (loss: 0.9977) | Gap: -1.11%
No improvement (2/10)

Epoch 34/60 (lr: 0.000300)
Train: 62.85% (loss: 0.9368) | Val: 56.26% (loss: 1.1040) | Gap: 6.59%
No improvement (3/10)

Epoch 35/60 (lr: 0.000300)
Train: 62.82% (loss: 0.9345) | Val: 63.92% (loss: 0.9855) | Gap: -1.10%
No improvement (4/10)

Epoch 36/60 (lr: 0.000300)
Train: 63.54% (loss: 0.9123) | Val: 64.01% (loss: 0.9912) | Gap: -0.46%
No improvement (5/10)

Epoch 37/60 (lr: 0.000300)
Train: 63.96% (loss: 0.9023) | Val: 64.26% (loss: 1.0726) | Gap: -0.30%
No improvement (6/10)

Epoch 38/60 (lr: 0.000300)
Train: 63.55% (loss: 0.9027) | Val: 64.72% (loss: 1.0116) | Gap: -1.17%
New best model saved! Val Acc: 64.72%

Epoch 39/60 (lr: 0.000300)
Train: 64.35% (loss: 0.8771) | Val: 63.21% (loss: 1.1238) | Gap: 1.14%
No improvement (1/10)

Epoch 40/60 (lr: 0.000300)
Train: 64.50% (loss: 0.8861) | Val: 65.78% (loss: 1.0140) | Gap: -1.28%
New best model saved! Val Acc: 65.78%

Epoch 41/60 (lr: 0.000300)
Train: 64.99% (loss: 0.8669) | Val: 65.25% (loss: 0.9997) | Gap: -0.26%
No improvement (1/10)

Epoch 42/60 (lr: 0.000300)
Train: 65.70% (loss: 0.8555) | Val: 64.74% (loss: 0.9622) | Gap: 0.95%
No improvement (2/10)

Epoch 43/60 (lr: 0.000300)
Train: 65.33% (loss: 0.8608) | Val: 66.30% (loss: 1.0776) | Gap: -0.97%
New best model saved! Val Acc: 66.30%

Epoch 44/60 (lr: 0.000300)
Train: 65.60% (loss: 0.8585) | Val: 66.18% (loss: 1.0654) | Gap: -0.58%
No improvement (1/10)

Epoch 45/60 (lr: 0.000300)
Train: 65.83% (loss: 0.8592) | Val: 67.21% (loss: 1.0378) | Gap: -1.38%
New best model saved! Val Acc: 67.21%

Epoch 46/60 (lr: 0.000300)
Train: 66.50% (loss: 0.8376) | Val: 65.99% (loss: 1.0093) | Gap: 0.52%
No improvement (1/10)

Epoch 47/60 (lr: 0.000300)
Train: 66.35% (loss: 0.8302) | Val: 66.62% (loss: 0.9729) | Gap: -0.27%
No improvement (2/10)

Epoch 48/60 (lr: 0.000300)
Train: 66.79% (loss: 0.8386) | Val: 64.72% (loss: 0.9859) | Gap: 2.07%
No improvement (3/10)

Epoch 49/60 (lr: 0.000300)
Train: 67.01% (loss: 0.8212) | Val: 66.45% (loss: 1.0607) | Gap: 0.57%
No improvement (4/10)

Epoch 50/60 (lr: 0.000300)
Train: 67.47% (loss: 0.8103) | Val: 65.73% (loss: 1.0532) | Gap: 1.74%
No improvement (5/10)

Epoch 51/60 (lr: 0.000300)
Train: 67.75% (loss: 0.7982) | Val: 66.95% (loss: 1.0727) | Gap: 0.79%
No improvement (6/10)

Epoch 52/60 (lr: 0.000300)
Train: 67.51% (loss: 0.8095) | Val: 66.79% (loss: 1.0408) | Gap: 0.72%
No improvement (7/10)

Epoch 53/60 (lr: 0.000300)
Train: 67.77% (loss: 0.7920) | Val: 62.17% (loss: 1.0406) | Gap: 5.60%
No improvement (8/10)

Epoch 54/60 (lr: 0.000300)
Train: 67.96% (loss: 0.7867) | Val: 65.88% (loss: 1.0277) | Gap: 2.08%
No improvement (9/10)

Epoch 55/60 (lr: 0.000300)
Train: 68.88% (loss: 0.7657) | Val: 66.05% (loss: 1.1311) | Gap: 2.83%
No improvement (10/10)

Early stopping after 55 epochs. Best val acc: 67.21%
Training complete!
Best Validation Accuracy: 67.21%
Training history saved to: /home/f/fernolend/emotion-recognition-cnn/results/v1/history.json
Training curves saved to: /home/f/fernolend/emotion-recognition-cnn/results/v1/visualizations/training_curves.png

Loading best model for final test evaluation...

Final Test Results:
Test Accuracy: 66.81%
Test Loss: 0.8768
Target not reached. Current: 66.81%, Target: 75%

==========================================
Running evaluation for model version: v1
==========================================
Using device: cuda
Loading model...
Evaluating model version: v1
Loading test data...
Dataset splits: train=18996, val=4748, test=5945
Testing...

Test Accuracy: 66.81%

Per-class results:
              precision    recall  f1-score   support

   happiness       0.87      0.87      0.87      1774
    surprise       0.80      0.74      0.77       831
     sadness       0.56      0.62      0.58      1247
       anger       0.53      0.69      0.60       958
     disgust       0.63      0.63      0.63       111
        fear       0.47      0.31      0.37      1024

    accuracy                           0.67      5945
   macro avg       0.64      0.64      0.64      5945
weighted avg       0.67      0.67      0.66      5945


Confusion matrix saved to /home/f/fernolend/emotion-recognition-cnn/results/v1/visualizations/confusion_matrix.png

v1 completed at: Fri Jan 30 20:41:13 CET 2026
==========================================

==========================================
Starting training for model version: v2
Time: Fri Jan 30 20:41:13 CET 2026
==========================================
Using device: cuda
GPU: NVIDIA GeForce RTX 2060 SUPER

Training model version: v2
Model name: V2 - SE Attention + 4 Blocks

==================================================
Configuration Summary
==================================================
  Learning Rate:     0.0001
  Weight Decay:      0.0001
  Dropout Rate:      0.5
  Epochs (max):      60
  Early Stop:        10 epochs
  Class Weights:     True
  Label Smoothing:   0.0
  LR Scheduler:      False
  Model Save Path:   /home/f/fernolend/emotion-recognition-cnn/results/v2/models/emotion_model.pth
==================================================

Loading data...
Dataset splits: train=18996, val=4748, test=5945
Train batches: 594, Val batches: 149, Test batches: 186

Initializing model...
Total parameters: 5,563,910
Trainable parameters: 5,563,910

Computing class weights...
Class weights (inverse frequency):
  happiness: 0.548 (7215 samples)
  surprise: 1.248 (3171 samples)
  sadness: 0.819 (4830 samples)
  anger: 0.991 (3995 samples)
  disgust: 9.076 (436 samples)
  fear: 0.966 (4097 samples)
Using weighted CrossEntropyLoss with label smoothing (0.0)
Optimizer: Adam (lr=0.0001, weight_decay=0.0001)
LR Scheduler: None

Training for up to 60 epochs (early stopping patience: 10)

Epoch 1/60 (lr: 0.000100)
Train: 24.41% (loss: 1.7573) | Val: 34.84% (loss: 1.6991) | Gap: -10.43%
New best model saved! Val Acc: 34.84%

Epoch 2/60 (lr: 0.000100)
Train: 33.92% (loss: 1.6448) | Val: 39.97% (loss: 1.6110) | Gap: -6.06%
New best model saved! Val Acc: 39.97%

Epoch 3/60 (lr: 0.000100)
Train: 41.79% (loss: 1.5254) | Val: 46.46% (loss: 1.4121) | Gap: -4.67%
New best model saved! Val Acc: 46.46%

Epoch 4/60 (lr: 0.000100)
Train: 44.54% (loss: 1.4366) | Val: 39.03% (loss: 1.5434) | Gap: 5.51%
No improvement (1/10)

Epoch 5/60 (lr: 0.000100)
Train: 48.81% (loss: 1.3619) | Val: 54.13% (loss: 1.2461) | Gap: -5.32%
New best model saved! Val Acc: 54.13%

Epoch 6/60 (lr: 0.000100)
Train: 50.68% (loss: 1.3011) | Val: 54.57% (loss: 1.2981) | Gap: -3.89%
New best model saved! Val Acc: 54.57%

Epoch 7/60 (lr: 0.000100)
Train: 52.86% (loss: 1.2477) | Val: 53.41% (loss: 1.1913) | Gap: -0.55%
No improvement (1/10)

Epoch 8/60 (lr: 0.000100)
Train: 53.94% (loss: 1.2123) | Val: 55.69% (loss: 1.1369) | Gap: -1.75%
New best model saved! Val Acc: 55.69%

Epoch 9/60 (lr: 0.000100)
Train: 55.36% (loss: 1.1736) | Val: 59.01% (loss: 1.1527) | Gap: -3.66%
New best model saved! Val Acc: 59.01%

Epoch 10/60 (lr: 0.000100)
Train: 56.40% (loss: 1.1501) | Val: 60.24% (loss: 1.0579) | Gap: -3.84%
New best model saved! Val Acc: 60.24%

Epoch 11/60 (lr: 0.000100)
Train: 57.69% (loss: 1.1237) | Val: 57.98% (loss: 1.1786) | Gap: -0.29%
No improvement (1/10)

Epoch 12/60 (lr: 0.000100)
Train: 58.31% (loss: 1.1010) | Val: 59.54% (loss: 1.1342) | Gap: -1.23%
No improvement (2/10)

Epoch 13/60 (lr: 0.000100)
Train: 59.19% (loss: 1.0729) | Val: 58.80% (loss: 1.0723) | Gap: 0.39%
No improvement (3/10)

Epoch 14/60 (lr: 0.000100)
Train: 60.04% (loss: 1.0466) | Val: 57.12% (loss: 1.0911) | Gap: 2.93%
No improvement (4/10)

Epoch 15/60 (lr: 0.000100)
Train: 60.07% (loss: 1.0375) | Val: 59.63% (loss: 1.1051) | Gap: 0.44%
No improvement (5/10)

Epoch 16/60 (lr: 0.000100)
Train: 60.58% (loss: 1.0108) | Val: 63.82% (loss: 1.0854) | Gap: -3.24%
New best model saved! Val Acc: 63.82%

Epoch 17/60 (lr: 0.000100)
Train: 61.81% (loss: 0.9972) | Val: 61.58% (loss: 1.0674) | Gap: 0.23%
No improvement (1/10)

Epoch 18/60 (lr: 0.000100)
Train: 62.13% (loss: 0.9856) | Val: 61.08% (loss: 0.9983) | Gap: 1.06%
No improvement (2/10)

Epoch 19/60 (lr: 0.000100)
Train: 62.57% (loss: 0.9710) | Val: 63.16% (loss: 1.0000) | Gap: -0.59%
No improvement (3/10)

Epoch 20/60 (lr: 0.000100)
Train: 62.81% (loss: 0.9622) | Val: 63.02% (loss: 1.0247) | Gap: -0.21%
No improvement (4/10)

Epoch 21/60 (lr: 0.000100)
Train: 63.63% (loss: 0.9432) | Val: 63.48% (loss: 0.9886) | Gap: 0.15%
No improvement (5/10)

Epoch 22/60 (lr: 0.000100)
Train: 64.01% (loss: 0.9261) | Val: 64.70% (loss: 0.9830) | Gap: -0.69%
New best model saved! Val Acc: 64.70%

Epoch 23/60 (lr: 0.000100)
Train: 64.33% (loss: 0.9139) | Val: 62.03% (loss: 1.0106) | Gap: 2.31%
No improvement (1/10)

Epoch 24/60 (lr: 0.000100)
Train: 64.61% (loss: 0.9041) | Val: 60.80% (loss: 1.0536) | Gap: 3.80%
No improvement (2/10)

Epoch 25/60 (lr: 0.000100)
Train: 65.36% (loss: 0.8861) | Val: 62.93% (loss: 0.9880) | Gap: 2.42%
No improvement (3/10)

Epoch 26/60 (lr: 0.000100)
Train: 65.61% (loss: 0.8814) | Val: 64.17% (loss: 0.9805) | Gap: 1.43%
No improvement (4/10)

Epoch 27/60 (lr: 0.000100)
Train: 65.71% (loss: 0.8727) | Val: 64.24% (loss: 0.9660) | Gap: 1.48%
No improvement (5/10)

Epoch 28/60 (lr: 0.000100)
Train: 66.42% (loss: 0.8463) | Val: 65.84% (loss: 1.0147) | Gap: 0.59%
New best model saved! Val Acc: 65.84%

Epoch 29/60 (lr: 0.000100)
Train: 66.56% (loss: 0.8552) | Val: 64.05% (loss: 1.0994) | Gap: 2.51%
No improvement (1/10)

Epoch 30/60 (lr: 0.000100)
Train: 66.78% (loss: 0.8360) | Val: 61.75% (loss: 1.1355) | Gap: 5.03%
No improvement (2/10)

Epoch 31/60 (lr: 0.000100)
Train: 67.27% (loss: 0.8227) | Val: 67.27% (loss: 0.9767) | Gap: -0.00%
New best model saved! Val Acc: 67.27%

Epoch 32/60 (lr: 0.000100)
Train: 67.32% (loss: 0.8254) | Val: 67.78% (loss: 0.9672) | Gap: -0.46%
New best model saved! Val Acc: 67.78%

Epoch 33/60 (lr: 0.000100)
Train: 67.72% (loss: 0.8122) | Val: 64.13% (loss: 0.9910) | Gap: 3.59%
No improvement (1/10)

Epoch 34/60 (lr: 0.000100)
Train: 68.26% (loss: 0.8011) | Val: 65.52% (loss: 0.9803) | Gap: 2.74%
No improvement (2/10)

Epoch 35/60 (lr: 0.000100)
Train: 68.47% (loss: 0.7997) | Val: 68.28% (loss: 0.9487) | Gap: 0.19%
New best model saved! Val Acc: 68.28%

Epoch 36/60 (lr: 0.000100)
Train: 69.56% (loss: 0.7740) | Val: 66.51% (loss: 0.9780) | Gap: 3.04%
No improvement (1/10)

Epoch 37/60 (lr: 0.000100)
Train: 68.82% (loss: 0.7809) | Val: 49.43% (loss: 1.3444) | Gap: 19.39%
No improvement (2/10)

Epoch 38/60 (lr: 0.000100)
Train: 68.98% (loss: 0.7777) | Val: 66.49% (loss: 1.0153) | Gap: 2.49%
No improvement (3/10)

Epoch 39/60 (lr: 0.000100)
Train: 69.33% (loss: 0.7761) | Val: 61.75% (loss: 1.1024) | Gap: 7.57%
No improvement (4/10)

Epoch 40/60 (lr: 0.000100)
Train: 69.78% (loss: 0.7602) | Val: 67.73% (loss: 0.9625) | Gap: 2.04%
No improvement (5/10)

Epoch 41/60 (lr: 0.000100)
Train: 70.41% (loss: 0.7395) | Val: 64.87% (loss: 1.0612) | Gap: 5.55%
No improvement (6/10)

Epoch 42/60 (lr: 0.000100)
Train: 70.50% (loss: 0.7380) | Val: 68.22% (loss: 1.0828) | Gap: 2.29%
No improvement (7/10)

Epoch 43/60 (lr: 0.000100)
Train: 70.89% (loss: 0.7300) | Val: 64.36% (loss: 1.1086) | Gap: 6.52%
No improvement (8/10)

Epoch 44/60 (lr: 0.000100)
Train: 71.44% (loss: 0.7176) | Val: 65.69% (loss: 1.0346) | Gap: 5.75%
No improvement (9/10)

Epoch 45/60 (lr: 0.000100)
Train: 71.00% (loss: 0.7282) | Val: 68.62% (loss: 0.9401) | Gap: 2.39%
New best model saved! Val Acc: 68.62%

Epoch 46/60 (lr: 0.000100)
Train: 71.63% (loss: 0.7192) | Val: 67.88% (loss: 0.9478) | Gap: 3.74%
No improvement (1/10)

Epoch 47/60 (lr: 0.000100)
Train: 72.06% (loss: 0.6895) | Val: 64.66% (loss: 1.0404) | Gap: 7.40%
No improvement (2/10)

Epoch 48/60 (lr: 0.000100)
Train: 71.83% (loss: 0.7041) | Val: 68.93% (loss: 1.0134) | Gap: 2.90%
New best model saved! Val Acc: 68.93%

Epoch 49/60 (lr: 0.000100)
Train: 72.57% (loss: 0.6903) | Val: 65.82% (loss: 1.0225) | Gap: 6.76%
No improvement (1/10)

Epoch 50/60 (lr: 0.000100)
Train: 72.35% (loss: 0.6930) | Val: 64.93% (loss: 1.0305) | Gap: 7.41%
No improvement (2/10)

Epoch 51/60 (lr: 0.000100)
Train: 72.85% (loss: 0.6762) | Val: 68.34% (loss: 1.1114) | Gap: 4.50%
No improvement (3/10)

Epoch 52/60 (lr: 0.000100)
Train: 72.77% (loss: 0.6852) | Val: 63.35% (loss: 1.0636) | Gap: 9.42%
No improvement (4/10)

Epoch 53/60 (lr: 0.000100)
Train: 73.07% (loss: 0.6736) | Val: 63.63% (loss: 1.0803) | Gap: 9.45%
No improvement (5/10)

Epoch 54/60 (lr: 0.000100)
Train: 73.70% (loss: 0.6628) | Val: 67.04% (loss: 1.0278) | Gap: 6.66%
No improvement (6/10)

Epoch 55/60 (lr: 0.000100)
Train: 74.01% (loss: 0.6640) | Val: 66.62% (loss: 1.0205) | Gap: 7.39%
No improvement (7/10)

Epoch 56/60 (lr: 0.000100)
Train: 74.27% (loss: 0.6306) | Val: 66.34% (loss: 1.1459) | Gap: 7.93%
No improvement (8/10)

Epoch 57/60 (lr: 0.000100)
Train: 74.06% (loss: 0.6467) | Val: 69.52% (loss: 0.9931) | Gap: 4.54%
New best model saved! Val Acc: 69.52%

Epoch 58/60 (lr: 0.000100)
Train: 74.94% (loss: 0.6250) | Val: 70.20% (loss: 1.1216) | Gap: 4.74%
New best model saved! Val Acc: 70.20%

Epoch 59/60 (lr: 0.000100)
Train: 75.32% (loss: 0.6216) | Val: 69.06% (loss: 1.0355) | Gap: 6.26%
No improvement (1/10)

Epoch 60/60 (lr: 0.000100)
Train: 75.20% (loss: 0.6173) | Val: 67.08% (loss: 0.9922) | Gap: 8.12%
No improvement (2/10)

Training complete!
Best Validation Accuracy: 70.20%
Training history saved to: /home/f/fernolend/emotion-recognition-cnn/results/v2/history.json
Training curves saved to: /home/f/fernolend/emotion-recognition-cnn/results/v2/visualizations/training_curves.png

Loading best model for final test evaluation...

Final Test Results:
Test Accuracy: 68.68%
Test Loss: 0.8678
Target not reached. Current: 68.68%, Target: 75%

==========================================
Running evaluation for model version: v2
==========================================
Using device: cuda
Loading model...
Evaluating model version: v2
Loading test data...
Dataset splits: train=18996, val=4748, test=5945
Testing...

Test Accuracy: 68.68%

Per-class results:
              precision    recall  f1-score   support

   happiness       0.91      0.84      0.87      1774
    surprise       0.80      0.78      0.79       831
     sadness       0.57      0.61      0.59      1247
       anger       0.59      0.62      0.60       958
     disgust       0.80      0.61      0.69       111
        fear       0.49      0.52      0.50      1024

    accuracy                           0.69      5945
   macro avg       0.69      0.66      0.68      5945
weighted avg       0.70      0.69      0.69      5945


Confusion matrix saved to /home/f/fernolend/emotion-recognition-cnn/results/v2/visualizations/confusion_matrix.png

v2 completed at: Fri Jan 30 21:30:52 CET 2026
==========================================

==========================================
Starting training for model version: v4
Time: Fri Jan 30 21:30:52 CET 2026
==========================================
Using device: cuda
GPU: NVIDIA GeForce RTX 2060 SUPER

Training model version: v4
Model name: Final Model with Attention

==================================================
Configuration Summary
==================================================
  Learning Rate:     0.0003
  Weight Decay:      0.0001
  Dropout Rate:      0.5
  Epochs (max):      60
  Early Stop:        10 epochs
  Class Weights:     True
  Label Smoothing:   0.1
  LR Scheduler:      True
  Model Save Path:   /home/f/fernolend/emotion-recognition-cnn/results/v4/models/emotion_model.pth
==================================================

Loading data...
Dataset splits: train=18996, val=4748, test=5945
Train batches: 594, Val batches: 149, Test batches: 186

Initializing model...
Total parameters: 5,563,602
Trainable parameters: 5,563,602

Computing class weights...
Class weights (inverse frequency):
  happiness: 0.548 (7215 samples)
  surprise: 1.248 (3171 samples)
  sadness: 0.819 (4830 samples)
  anger: 0.991 (3995 samples)
  disgust: 9.076 (436 samples)
  fear: 0.966 (4097 samples)
Using weighted CrossEntropyLoss with label smoothing (0.1)
Optimizer: Adam (lr=0.0003, weight_decay=0.0001)
LR Scheduler: ReduceLROnPlateau (factor=0.5, patience=5)

Training for up to 60 epochs (early stopping patience: 10)

Epoch 1/60 (lr: 0.000300)
Train: 15.19% (loss: 2.0923) | Val: 11.84% (loss: 1.9988) | Gap: 3.35%
New best model saved! Val Acc: 11.84%

Epoch 2/60 (lr: 0.000300)
Train: 10.89% (loss: 2.0334) | Val: 3.75% (loss: 1.9857) | Gap: 7.14%
No improvement (1/10)

Epoch 3/60 (lr: 0.000300)
Train: 11.63% (loss: 1.9834) | Val: 13.73% (loss: 1.9317) | Gap: -2.10%
New best model saved! Val Acc: 13.73%

Epoch 4/60 (lr: 0.000300)
Train: 21.23% (loss: 1.8864) | Val: 32.96% (loss: 1.9036) | Gap: -11.74%
New best model saved! Val Acc: 32.96%

Epoch 5/60 (lr: 0.000300)
Train: 36.99% (loss: 1.7718) | Val: 36.37% (loss: 1.7316) | Gap: 0.62%
New best model saved! Val Acc: 36.37%

Epoch 6/60 (lr: 0.000300)
Train: 44.88% (loss: 1.6955) | Val: 40.84% (loss: 1.6486) | Gap: 4.04%
New best model saved! Val Acc: 40.84%

Epoch 7/60 (lr: 0.000300)
Train: 47.11% (loss: 1.6552) | Val: 55.03% (loss: 1.5758) | Gap: -7.92%
New best model saved! Val Acc: 55.03%

Epoch 8/60 (lr: 0.000300)
Train: 49.56% (loss: 1.6244) | Val: 58.30% (loss: 1.5340) | Gap: -8.74%
New best model saved! Val Acc: 58.30%

Epoch 9/60 (lr: 0.000300)
Train: 52.64% (loss: 1.5832) | Val: 57.52% (loss: 1.5249) | Gap: -4.88%
No improvement (1/10)

Epoch 10/60 (lr: 0.000300)
Train: 54.03% (loss: 1.5604) | Val: 59.29% (loss: 1.4864) | Gap: -5.26%
New best model saved! Val Acc: 59.29%

Epoch 11/60 (lr: 0.000300)
Train: 56.26% (loss: 1.5407) | Val: 55.83% (loss: 1.4939) | Gap: 0.43%
No improvement (1/10)

Epoch 12/60 (lr: 0.000300)
Train: 57.44% (loss: 1.5269) | Val: 62.87% (loss: 1.4963) | Gap: -5.42%
New best model saved! Val Acc: 62.87%

Epoch 13/60 (lr: 0.000300)
Train: 58.60% (loss: 1.5017) | Val: 61.50% (loss: 1.4741) | Gap: -2.90%
No improvement (1/10)

Epoch 14/60 (lr: 0.000300)
Train: 58.87% (loss: 1.4899) | Val: 64.01% (loss: 1.4602) | Gap: -5.14%
New best model saved! Val Acc: 64.01%

Epoch 15/60 (lr: 0.000300)
Train: 60.03% (loss: 1.4755) | Val: 57.14% (loss: 1.4625) | Gap: 2.89%
No improvement (1/10)

Epoch 16/60 (lr: 0.000300)
Train: 61.15% (loss: 1.4647) | Val: 66.18% (loss: 1.4277) | Gap: -5.02%
New best model saved! Val Acc: 66.18%

Epoch 17/60 (lr: 0.000300)
Train: 62.05% (loss: 1.4533) | Val: 63.71% (loss: 1.4263) | Gap: -1.66%
No improvement (1/10)

Epoch 18/60 (lr: 0.000300)
Train: 62.32% (loss: 1.4408) | Val: 65.94% (loss: 1.4054) | Gap: -3.62%
No improvement (2/10)

Epoch 19/60 (lr: 0.000300)
Train: 62.77% (loss: 1.4384) | Val: 66.13% (loss: 1.4341) | Gap: -3.36%
No improvement (3/10)

Epoch 20/60 (lr: 0.000300)
Train: 63.90% (loss: 1.4144) | Val: 67.35% (loss: 1.4028) | Gap: -3.45%
New best model saved! Val Acc: 67.35%

Epoch 21/60 (lr: 0.000300)
Train: 64.21% (loss: 1.4134) | Val: 64.49% (loss: 1.4310) | Gap: -0.28%
No improvement (1/10)

Epoch 22/60 (lr: 0.000300)
Train: 64.97% (loss: 1.3944) | Val: 66.95% (loss: 1.3939) | Gap: -1.99%
No improvement (2/10)

Epoch 23/60 (lr: 0.000300)
Train: 64.62% (loss: 1.3945) | Val: 65.96% (loss: 1.4262) | Gap: -1.34%
No improvement (3/10)

Epoch 24/60 (lr: 0.000300)
Train: 66.23% (loss: 1.3746) | Val: 65.21% (loss: 1.4303) | Gap: 1.02%
No improvement (4/10)

Epoch 25/60 (lr: 0.000300)
Train: 66.37% (loss: 1.3783) | Val: 67.57% (loss: 1.3895) | Gap: -1.19%
New best model saved! Val Acc: 67.57%

Epoch 26/60 (lr: 0.000300)
Train: 67.04% (loss: 1.3640) | Val: 69.23% (loss: 1.3828) | Gap: -2.19%
New best model saved! Val Acc: 69.23%

Epoch 27/60 (lr: 0.000300)
Train: 67.24% (loss: 1.3640) | Val: 66.91% (loss: 1.3944) | Gap: 0.32%
No improvement (1/10)

Epoch 28/60 (lr: 0.000300)
Train: 67.82% (loss: 1.3485) | Val: 68.34% (loss: 1.3746) | Gap: -0.53%
No improvement (2/10)

Epoch 29/60 (lr: 0.000300)
Train: 68.41% (loss: 1.3424) | Val: 69.40% (loss: 1.3713) | Gap: -0.98%
New best model saved! Val Acc: 69.40%

Epoch 30/60 (lr: 0.000300)
Train: 68.71% (loss: 1.3390) | Val: 66.32% (loss: 1.3902) | Gap: 2.39%
No improvement (1/10)

Epoch 31/60 (lr: 0.000300)
Train: 69.03% (loss: 1.3289) | Val: 69.52% (loss: 1.3824) | Gap: -0.49%
New best model saved! Val Acc: 69.52%

Epoch 32/60 (lr: 0.000300)
Train: 70.09% (loss: 1.3183) | Val: 66.36% (loss: 1.4100) | Gap: 3.73%
No improvement (1/10)

Epoch 33/60 (lr: 0.000300)
Train: 69.90% (loss: 1.3103) | Val: 69.36% (loss: 1.3546) | Gap: 0.55%
No improvement (2/10)

Epoch 34/60 (lr: 0.000300)
Train: 69.98% (loss: 1.3112) | Val: 68.66% (loss: 1.3562) | Gap: 1.32%
No improvement (3/10)

Epoch 35/60 (lr: 0.000300)
Train: 70.65% (loss: 1.2954) | Val: 68.98% (loss: 1.3765) | Gap: 1.67%
No improvement (4/10)

Epoch 36/60 (lr: 0.000300)
Train: 70.57% (loss: 1.3002) | Val: 67.63% (loss: 1.3690) | Gap: 2.94%
No improvement (5/10)

Epoch 37/60 (lr: 0.000300)
Train: 71.74% (loss: 1.2846) | Val: 69.73% (loss: 1.3619) | Gap: 2.00%
New best model saved! Val Acc: 69.73%

Epoch 38/60 (lr: 0.000300)
Train: 71.84% (loss: 1.2806) | Val: 69.00% (loss: 1.3619) | Gap: 2.84%
No improvement (1/10)

Epoch 39/60 (lr: 0.000300)
Train: 72.07% (loss: 1.2667) | Val: 71.12% (loss: 1.3552) | Gap: 0.94%
New best model saved! Val Acc: 71.12%

Epoch 40/60 (lr: 0.000300)
Train: 72.18% (loss: 1.2733) | Val: 70.94% (loss: 1.3474) | Gap: 1.24%
No improvement (1/10)

Epoch 41/60 (lr: 0.000300)
Train: 73.08% (loss: 1.2569) | Val: 69.90% (loss: 1.3447) | Gap: 3.18%
No improvement (2/10)

Epoch 42/60 (lr: 0.000300)
Train: 74.03% (loss: 1.2423) | Val: 71.19% (loss: 1.3686) | Gap: 2.84%
New best model saved! Val Acc: 71.19%

Epoch 43/60 (lr: 0.000300)
Train: 74.01% (loss: 1.2448) | Val: 69.17% (loss: 1.3660) | Gap: 4.84%
No improvement (1/10)

Epoch 44/60 (lr: 0.000300)
Train: 74.13% (loss: 1.2387) | Val: 70.01% (loss: 1.3625) | Gap: 4.12%
No improvement (2/10)

Epoch 45/60 (lr: 0.000300)
Train: 74.56% (loss: 1.2285) | Val: 69.02% (loss: 1.3932) | Gap: 5.54%
No improvement (3/10)

Epoch 46/60 (lr: 0.000300)
Train: 74.73% (loss: 1.2323) | Val: 70.09% (loss: 1.3608) | Gap: 4.64%
No improvement (4/10)

Epoch 47/60 (lr: 0.000300)
Train: 74.89% (loss: 1.2278) | Val: 70.28% (loss: 1.3571) | Gap: 4.61%
No improvement (5/10)

Epoch 48/60 (lr: 0.000300)
Train: 75.39% (loss: 1.2228) | Val: 71.42% (loss: 1.3546) | Gap: 3.97%
New best model saved! Val Acc: 71.42%

Epoch 49/60 (lr: 0.000300)
Train: 76.02% (loss: 1.2109) | Val: 71.04% (loss: 1.3556) | Gap: 4.98%
No improvement (1/10)

Epoch 50/60 (lr: 0.000300)
Train: 76.26% (loss: 1.2057) | Val: 71.31% (loss: 1.3570) | Gap: 4.94%
No improvement (2/10)

Epoch 51/60 (lr: 0.000300)
Train: 76.03% (loss: 1.2013) | Val: 71.59% (loss: 1.3417) | Gap: 4.44%
New best model saved! Val Acc: 71.59%

Epoch 52/60 (lr: 0.000300)
Train: 76.63% (loss: 1.2024) | Val: 70.03% (loss: 1.3670) | Gap: 6.60%
No improvement (1/10)

Epoch 53/60 (lr: 0.000300)
Train: 76.99% (loss: 1.1918) | Val: 69.97% (loss: 1.3637) | Gap: 7.02%
No improvement (2/10)

Epoch 54/60 (lr: 0.000300)
Train: 78.15% (loss: 1.1723) | Val: 70.41% (loss: 1.3648) | Gap: 7.74%
No improvement (3/10)

Epoch 55/60 (lr: 0.000300)
Train: 77.65% (loss: 1.1757) | Val: 70.09% (loss: 1.3689) | Gap: 7.56%
No improvement (4/10)

Epoch 56/60 (lr: 0.000300)
Train: 78.48% (loss: 1.1717) | Val: 70.81% (loss: 1.3672) | Gap: 7.68%
No improvement (5/10)

Epoch 57/60 (lr: 0.000300)
Train: 78.15% (loss: 1.1650) | Val: 70.01% (loss: 1.3951) | Gap: 8.14%
No improvement (6/10)

Epoch 58/60 (lr: 0.000150)
Train: 81.19% (loss: 1.1166) | Val: 72.75% (loss: 1.3478) | Gap: 8.44%
New best model saved! Val Acc: 72.75%

Epoch 59/60 (lr: 0.000150)
Train: 82.31% (loss: 1.0950) | Val: 72.16% (loss: 1.3701) | Gap: 10.15%
No improvement (1/10)

Epoch 60/60 (lr: 0.000150)
Train: 82.92% (loss: 1.0850) | Val: 72.37% (loss: 1.3583) | Gap: 10.55%
No improvement (2/10)

Training complete!
Best Validation Accuracy: 72.75%
Training history saved to: /home/f/fernolend/emotion-recognition-cnn/results/v4/history.json
Training curves saved to: /home/f/fernolend/emotion-recognition-cnn/results/v4/visualizations/training_curves.png

Loading best model for final test evaluation...

Final Test Results:
Test Accuracy: 71.74%
Test Loss: 1.4649
Target not reached. Current: 71.74%, Target: 75%

==========================================
Running evaluation for model version: v4
==========================================
Using device: cuda
Loading model...
Evaluating model version: v4
Loading test data...
Dataset splits: train=18996, val=4748, test=5945
Testing...

Test Accuracy: 71.74%

Per-class results:
              precision    recall  f1-score   support

   happiness       0.92      0.88      0.90      1774
    surprise       0.80      0.82      0.81       831
     sadness       0.62      0.65      0.64      1247
       anger       0.62      0.62      0.62       958
     disgust       0.52      0.68      0.59       111
        fear       0.56      0.52      0.54      1024

    accuracy                           0.72      5945
   macro avg       0.67      0.70      0.68      5945
weighted avg       0.72      0.72      0.72      5945


Confusion matrix saved to /home/f/fernolend/emotion-recognition-cnn/results/v4/visualizations/confusion_matrix.png

v4 completed at: Fri Jan 30 22:26:32 CET 2026
==========================================

==========================================
All jobs finished at: Fri Jan 30 22:26:32 CET 2026
Trained versions: v1 v2 v4
==========================================

==========================================
Results Summary
==========================================

=== v1 ===
Best Val Acc: 67.21%
Test Acc: 66.81%
Epochs trained: 55

=== v2 ===
Best Val Acc: 70.20%
Test Acc: 68.68%
Epochs trained: 60

=== v4 ===
Best Val Acc: 72.75%
Test Acc: 71.74%
Epochs trained: 60
